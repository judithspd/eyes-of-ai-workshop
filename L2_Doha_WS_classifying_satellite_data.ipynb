{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp6VaOsvVoOY"
      },
      "source": [
        "# Automatic Land Cover Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Eb0MjcRX6fM"
      },
      "source": [
        "![Dataset sample](https://i.ibb.co/ZYfpM52/bak-sat.png)\n",
        "\n",
        "*Helber, P., Bischke, B., Dengel, A., & Borth, D. (2018). EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification [Data set]. In EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification (Vol. 12, No. 7, pp. 2217-2226). Zenodo. https://doi.org/10.5281/zenodo.7711810*\n",
        "\n",
        "In this exercise, we will use an open dataset for land use classification. This dataset is derived from the Sentinel-2 satellite and is available in both RGB format (which we will use) and TIFF format, with the satellite's 13 bands, providing much more information but making processing computationally more complex.\n",
        "\n",
        "To analyze, process, or extract any added value from remote sensing data, it is necessary to choose a working environment and have data availability. For this, we have different options.\n",
        "\n",
        "### Working Environments\n",
        "We can choose Geographic Information Systems, tools from various agencies such as SNAP, services like Google Earth Engine (GEE), or use our own code. More and more examples are available in *Jupyter notebooks*, which is what we will use. To run them, we can install Jupyter on our PC or use a cloud service, such as:\n",
        "\n",
        "* Jupyterhub Sentinel Hub: https://jupyterhub.dataspace.copernicus.eu/ (registration required)\n",
        "* Google Colab\n",
        "\n",
        "### Data\n",
        "Depending on who generates the data and the available instruments, sometimes we need to access data differently. A very flexible service is GEE, which offers data from various sources. However, if we learn to select and download any type of data, we will have much more flexibility. In the second part of the exercise, we will focus on downloading Sentinel-2 data using the *Sentinel Hub Process API* service.\n",
        "\n",
        "We will see how to use some configuration parameters to obtain preprocessed or raw products. For more information, you can check the [official documentation](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Process.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfUF1pokVun8"
      },
      "source": [
        "### Imports\n",
        "Programming languages come with libraries, which are sets of functions that facilitate performing various tasks, such as loading images or designing neural networks. Among the ones we will use are TensorFlow (for training neural networks and other AI techniques), matplotlib (for plots), and numpy (for data management), among others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMUTd947r4hZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw00zYRpVxEG"
      },
      "source": [
        "In the following block, we download the available data in RGB format and extract it. If you look at the generated folder, it is divided into different folders according to the type of land cover (10 types or classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m91Q4C_ar78Z"
      },
      "outputs": [],
      "source": [
        "# Descargar el dataset EuroSAT\n",
        "!wget --no-check-certificate -O EuroSAT.zip https://madm.dfki.de/files/sentinel/EuroSAT.zip\n",
        "\n",
        "# Descomprimir el archivo\n",
        "with zipfile.ZipFile('EuroSAT.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('EuroSAT')\n",
        "\n",
        "# Definir el directorio del dataset\n",
        "dataset_dir = 'EuroSAT/2750'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk99g6kWZmdo"
      },
      "source": [
        "In the following block, we prepare \"data generators\" that read the images we have downloaded and prepare them for training the AI model.\n",
        "\n",
        "We will split the data into training (80%) and validation (20%) sets. This will help us check during each epoch (or weight calculation iteration) whether the network we are training is overfitting or not. \"Overfitting\" occurs when the network learns very well to differentiate classes within the training data but does not perform as well with new data. What we want is to avoid this so we can use our network for other images or generalize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eNidrd2r8A5"
      },
      "outputs": [],
      "source": [
        "# Crear generadores de datos para entrenamiento y validación\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m70dMqUZjvm"
      },
      "source": [
        "Now it's time to configure our network. One of the things we can do is **transfer learning**, which involves starting from a pre-existing generic model (e.g., for image classification) and fine-tuning it for our specific case of satellite images.\n",
        "\n",
        "For practical purposes, we will build a network from scratch by adding different layers. Since we are working with image data, convolutional neural networks usually perform well, so we will add the following layers:\n",
        "\n",
        "**Conv2D Layer**\n",
        "\n",
        "- **Type:** Convolutional  \n",
        "- **Description:** This is the first convolutional layer, which applies 32 filters (or kernels) of size 3x3 over the input image.  \n",
        "- **Function:** Detects local features in the image, such as edges, textures, etc.\n",
        "\n",
        "**MaxPooling2D Layer**\n",
        "\n",
        "- **Description:** Reduces the dimensionality of the input (image) by taking the maximum value from each 2x2 block.  \n",
        "- **Function:** Decreases the spatial resolution of detected features, reducing the number of parameters and computational load.\n",
        "\n",
        "**Dropout Layer**\n",
        "\n",
        "- **Description:** During training, this layer randomly \"turns off\" 25% of the units (neurons) to prevent overfitting.  \n",
        "- **Function:** Helps prevent the model from fitting too closely to the training data, improving its generalization ability.\n",
        "\n",
        "**Flatten Layer**\n",
        "\n",
        "- **Description:** Converts the 2D (matrix) output into a 1D (vector) form so it can be fed into densely connected layers.  \n",
        "- **Function:** Prepares data for the dense (fully connected) layers.\n",
        "\n",
        "**Dense Layer**\n",
        "\n",
        "- **Function:** Processes the extracted features and learns non-linear combinations of these features.\n",
        "\n",
        "**Dense(13, activation='softmax') Layer**\n",
        "\n",
        "- **Description:** Output layer with 13 units, one for each class in the EuroSAT dataset.  \n",
        "- **Function:** Produces a probability distribution over the 13 classes.  \n",
        "- **Activation:** Softmax, which ensures that the sum of the outputs is 1, representing a probability for each class.\n",
        "\n",
        "Feel free to try adding or removing layers and observe what happens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8GN-McFZj5c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Definir la arquitectura del modelo\n",
        "num_classes = len(train_generator.class_indices)  # Get the actual number of classes\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(???filters???, (3, 3), activation='relu', input_shape=??image shape and bands),\n",
        "    ....\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r2bu67kbG6o"
      },
      "source": [
        "It's time for training. The model is divided into different **epochs**, where the data passes through the network and certain weights are calculated to minimize a loss function.\n",
        "\n",
        "For each epoch, 80% of the data is used for training, and 20% is used to check if the model performs well on data it hasn't seen before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGUjZKE4r8Dg"
      },
      "outputs": [],
      "source": [
        "# Entrenar el modelo con el callback de TensorBoard\n",
        "history = model.fit(...)\n",
        "\n",
        "# Evaluar el modelo\n",
        "loss, accuracy = model.evaluate(validation_generator, verbose=2)\n",
        "print(f'Validation accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "model.save('eurosat_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaf-vuL9bfqS"
      },
      "source": [
        "Once the training is complete, we can check for overfitting by displaying various graphs.\n",
        "\n",
        "If there is a significant difference between the accuracy and the loss function for the training and validation data—where the validation data performs worse—there may be overfitting.\n",
        "\n",
        "This can be addressed in different ways, such as \"disconnecting\" parts of the network layers using **Dropout**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWEKf41Br8F4"
      },
      "outputs": [],
      "source": [
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Loss')\n",
        "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMOAVtSwb_-F"
      },
      "source": [
        "## Model evaluation\n",
        "In addition to the **accuracy** that the model provides, one of the key plots used to evaluate AI models is the **confusion matrix**.\n",
        "\n",
        "For a test dataset, we will use the network to predict the class of each sample, knowing its true class. The matrix shows the relationship between the actual and predicted values, displaying the number of instances for each case (actual vs. predicted).\n",
        "\n",
        "For a well-performing model, it is expected that the diagonal of the matrix is clearly highlighted, indicating correct predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj9b7-HTr8IR"
      },
      "outputs": [],
      "source": [
        "# Obtener todas las imágenes y etiquetas del conjunto de validación\n",
        "# Iterar sobre el generador para obtener todas las muestras\n",
        "validation_images = []\n",
        "validation_labels = []\n",
        "for _ in range(len(validation_generator)):\n",
        "    batch_images, batch_labels = next(validation_generator)\n",
        "    validation_images.append(batch_images)\n",
        "    validation_labels.append(batch_labels)\n",
        "\n",
        "# Concatenar los lotes en un solo array\n",
        "validation_images = np.concatenate(validation_images, axis=0)\n",
        "validation_labels = np.concatenate(validation_labels, axis=0)\n",
        "\n",
        "# Seleccionar aleatoriamente un subconjunto de las imágenes de validación\n",
        "num_samples = 1000  # Número de muestras aleatorias para la matriz de confusión\n",
        "# Asegurarse de que num_samples no sea mayor que el número de imágenes disponibles\n",
        "num_samples = min(num_samples, len(validation_images))\n",
        "indices = np.random.choice(len(validation_images), num_samples, replace=False)\n",
        "random_images = validation_images[indices]\n",
        "random_labels = validation_labels[indices]\n",
        "\n",
        "# Hacer predicciones sobre estas imágenes aleatorias\n",
        "predictions = model.predict(random_images)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(random_labels, axis=1)\n",
        "\n",
        "# Generar la matriz de confusión\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Visualizar la matriz de confusión\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=validation_generator.class_indices.keys(), yticklabels=validation_generator.class_indices.keys())\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcqYmkIscldX"
      },
      "source": [
        "We can also visually evaluate the model by randomly displaying an image to compare the actual and predicted class.\n",
        "\n",
        "This allows us to better understand how well the model performs with real data and to identify potential misclassifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gykbe1jtcs3P"
      },
      "outputs": [],
      "source": [
        "# Imprimir el reporte de clasificación\n",
        "print(classification_report(true_labels, predicted_labels, target_names=validation_generator.class_indices.keys()))\n",
        "\n",
        "# Mostrar algunas imágenes con sus etiquetas verdaderas y predichas\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.imshow(random_images[i])\n",
        "    true_label = list(validation_generator.class_indices.keys())[true_labels[i]]\n",
        "    predicted_label = list(validation_generator.class_indices.keys())[predicted_labels[i]]\n",
        "    plt.title(f'True: {true_label}\\nPred: {predicted_label}')\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqbnsKRCc0LR"
      },
      "source": [
        "## Model Usage: Data Download\n",
        "\n",
        "Now let's use the model to make some predictions. In this case, we will download an image from **Sentinel Hub** and test different parts of it.\n",
        "\n",
        "For this, we need the **SentinelHub** library:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIwyDhAi6uKX"
      },
      "outputs": [],
      "source": [
        "!pip install sentinelhub\n",
        "from sentinelhub import (\n",
        "    SHConfig,\n",
        "    CRS,\n",
        "    BBox,\n",
        "    DataCollection,\n",
        "    DownloadRequest,\n",
        "    MimeType,\n",
        "    MosaickingOrder,\n",
        "    SentinelHubDownloadClient,\n",
        "    SentinelHubRequest,\n",
        "    bbox_to_dimensions,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5LZrJz9V4ja"
      },
      "source": [
        "### Credentials\n",
        "\n",
        "If you haven't done so yet, the first step is to register on the [Copernicus Data Space](https://identity.dataspace.copernicus.eu/auth/realms/CDSE/login-actions/registration?client_id=sh-a696e3be-b074-4baa-9e76-b10bee279c85&tab_id=Ns0w8gGsZac).\n",
        "\n",
        "The credentials for Sentinel Hub services (`client_id` and `client_secret`) can be obtained from your [Dashboard](https://shapps.dataspace.copernicus.eu/dashboard/#/). In the User Settings, you can create a new OAuth Client to generate these credentials. For more detailed instructions, visit the relevant [documentation page](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Overview/Authentication.html).\n",
        "\n",
        "Now that you have your `client_id` and `client_secret`, it is recommended to set up a new profile in your **Sentinel Hub Python package**. Instructions on how to configure your Sentinel Hub Python package can be found [here](https://sentinelhub-py.readthedocs.io/en/latest/configure.html). By following these instructions, you can create a specific profile to use the package for accessing the data collections from the **Copernicus Data Space Ecosystem**. This is useful because changes in the configuration class are usually temporary in a notebook, and by saving the configuration in your profile, you won’t need to generate new credentials or overwrite/change the default profile each time you run or write a new Jupyter Notebook.\n",
        "\n",
        "If this is your first time using the Sentinel Hub Python package for the **Copernicus Data Space Ecosystem**, you must create a specific profile for it. You can do this in the following cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHkqwjrU6uPY"
      },
      "outputs": [],
      "source": [
        "config = SHConfig()\n",
        "config.sh_client_id = 'sh-7b087513-8281-4caf-9120-f26c486a06e2'\n",
        "config.sh_client_secret = 'isk7TcXlC1U4iScfSCLcqFnAa0Y4qr4d'\n",
        "config.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
        "config.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n",
        "config.sh_auth_base_url=\"https://services.sentinel-hub.com\",\n",
        "config.save(\"uimp_24\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdtXTVdwV7hl"
      },
      "source": [
        "## Choosing Area of Interest\n",
        "\n",
        "We are going to download a Sentinel-2 image of the [Bay of Santander](https://en.wikipedia.org/wiki/Bay_of_Santander) like the one shown below (captured by Sentinel-2 on 2024-04-20):  \n",
        "![santander.png](https://i.ibb.co/bF6mtYW/sdr-sat.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXBBSgaqWWdv"
      },
      "source": [
        "The selected box in `WGS84` coordinates is `[-3.855858, 43.395070, -3.721447, 43.499383]` (longitude and latitude coordinates from the bottom-left to the top-right).\n",
        "\n",
        "If you want to select another area, you can use tools like [bboxfinder](http://bboxfinder.com/).\n",
        "\n",
        "All server requests require an area represented by a `sentinelhub.geometry.BBox` instance with the corresponding reference system (`sentinelhub.constants.CRS`).\n",
        "\n",
        "Once the bounding box is defined, you can initialize `BBox` with the area of interest. Using the `bbox_to_dimensions` function, we can calculate the exact resolution of the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYMaF2sg6uSh"
      },
      "outputs": [],
      "source": [
        "bahia_coords_wgs84 = (-3.855858, 43.395070, -3.721447, 43.499383)\n",
        "resolution = 10\n",
        "bahia_bbox = BBox(bbox=bahia_coords_wgs84, crs=CRS.WGS84)\n",
        "bahia_size = bbox_to_dimensions(bahia_bbox, resolution=resolution)\n",
        "\n",
        "print(f\"Image shape at {resolution} m resolution: {bahia_size} pixels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JPptlaSdgvs"
      },
      "source": [
        "In the documentation, you can find many configuration parameters for downloading data.\n",
        "\n",
        "In this case, we will request the **Red**, **Green**, and **Blue (RGB)** bands, which are the ones we used to train our model.\n",
        "\n",
        "However, we can also train and use the model with all available bands.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF8sz2Le6uVP"
      },
      "outputs": [],
      "source": [
        "evalscript_true_color = \"\"\"\n",
        "    //VERSION=3\n",
        "\n",
        "    function setup() {\n",
        "        return {\n",
        "            input: [{\n",
        "                bands: [\"B02\", \"B03\", \"B04\"]\n",
        "            }],\n",
        "            output: {\n",
        "                bands: 3\n",
        "            }\n",
        "        };\n",
        "    }\n",
        "\n",
        "    function evaluatePixel(sample) {\n",
        "        return [sample.B04, sample.B03, sample.B02];\n",
        "    }\n",
        "\"\"\"\n",
        "\n",
        "request_true_color = SentinelHubRequest(\n",
        "    evalscript=evalscript_true_color,\n",
        "    input_data=[\n",
        "        SentinelHubRequest.input_data(\n",
        "            data_collection=DataCollection.SENTINEL2_L1C.define_from(\n",
        "                \"s2l1c\", service_url=config.sh_base_url\n",
        "            ),\n",
        "            time_interval=(\"2024-04-19\", \"2024-04-21\"),\n",
        "        )\n",
        "    ],\n",
        "    responses=[SentinelHubRequest.output_response(\"default\", MimeType.JPG)],\n",
        "    bbox=bahia_bbox,\n",
        "    size=bahia_size,\n",
        "    config=config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kHuOtwS7Kg5"
      },
      "outputs": [],
      "source": [
        "true_color_imgs = request_true_color.get_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfI4flQ-d3s-"
      },
      "source": [
        "Let's display the downloaded image using a helper function called `plot_image`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysvHzBcJ7KjS"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Utility functions used by example notebooks\n",
        "\"\"\"\n",
        "from typing import Any, Optional, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def plot_image(\n",
        "    image: np.ndarray,\n",
        "    factor: float = 1.0,\n",
        "    clip_range: Optional[Tuple[float, float]] = None,\n",
        "    **kwargs: Any\n",
        ") -> None:\n",
        "    \"\"\"Utility function for plotting RGB images.\"\"\"\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 15))\n",
        "    if clip_range is not None:\n",
        "        ax.imshow(np.clip(image * factor, *clip_range), **kwargs)\n",
        "    else:\n",
        "        ax.imshow(image * factor, **kwargs)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv7LV44r7Klw"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"Returned data is of type = {type(true_color_imgs)} and length {len(true_color_imgs)}.\"\n",
        ")\n",
        "print(\n",
        "    f\"Single element in the list is of type {type(true_color_imgs[-1])} and has shape {true_color_imgs[-1].shape}\"\n",
        ")\n",
        "\n",
        "image = true_color_imgs[0]\n",
        "print(f\"Image type: {image.dtype}\")\n",
        "\n",
        "# plot function\n",
        "# factor 1/255 to scale between 0-1\n",
        "# factor 3.5 to increase brightness\n",
        "plot_image(image, factor=3.0 / 255, clip_range=(0, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZp5pJdEeC4H"
      },
      "source": [
        "The following code takes the downloaded image and selects random 64x64 patches, which is the resolution we used to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_Gd1dxL7KoZ"
      },
      "outputs": [],
      "source": [
        "!pip install Pillow\n",
        "from PIL import Image\n",
        "\n",
        "def load_and_split_image(image_array, target_size=(64, 64), num_crops=9):\n",
        "    # Convert NumPy array to PIL Image\n",
        "    image = Image.fromarray(image_array.astype(np.uint8))  # Assuming image_array is uint8 type\n",
        "\n",
        "    height, width = image.size  # Use PIL's size attribute\n",
        "    crops = []\n",
        "    for _ in range(num_crops):\n",
        "        # Seleccionar aleatoriamente la esquina superior izquierda del recorte\n",
        "        left = np.random.randint(0, width - target_size[0])\n",
        "        top = np.random.randint(0, height - target_size[1])\n",
        "        right = left + target_size[0]\n",
        "        bottom = top + target_size[1]\n",
        "\n",
        "        # Recortar la imagen y convertirla a un array de numpy\n",
        "        crop = image.crop((left, top, right, bottom))\n",
        "        crop_array = np.array(crop) * 3.0 / 255.0  # Normalizar los valores de los píxeles\n",
        "        crops.append(crop_array)\n",
        "\n",
        "    return np.array(crops)\n",
        "\n",
        "# Assuming 'image' is the NumPy array you want to process\n",
        "crops = load_and_split_image(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf39DA7VeQNI"
      },
      "source": [
        "We predict the categories of the different patches and display them with the predicted label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVZlYnZC7Kqx"
      },
      "outputs": [],
      "source": [
        "# Hacer predicciones sobre los trozos\n",
        "predictions = model.predict(crops)\n",
        "\n",
        "# Obtener las etiquetas predichas para cada trozo\n",
        "predicted_labels = np.argmax(predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poHv5A2PCOXT"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "# Encontrar la etiqueta más frecuente entre las predicciones\n",
        "final_prediction = mode(predicted_labels).mode\n",
        "\n",
        "# Obtener el nombre de la clase predicha\n",
        "predicted_class = list(validation_generator.class_indices.keys())[final_prediction] # The variable final_prediction is now a scalar and does not need to be indexed.\n",
        "\n",
        "print(f'Predicted class for the image: {predicted_class}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2ZjMUsIO1l2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(len(crops)):\n",
        "    plt.subplot(3, 3, i+1)  # Crear una cuadrícula de 3x3\n",
        "    plt.imshow(crops[i])\n",
        "    predicted_class = list(validation_generator.class_indices.keys())[predicted_labels[i]]\n",
        "    plt.title(f'Pred: {predicted_class}')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}